#!/usr/bin/env python3

from __future__ import annotations
import os
import sys
import json
import time
import re
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

# third-party
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("Install openai (new v1+ client): pip install openai")

try:
    import tiktoken
except Exception:
    raise SystemExit("Install tiktoken: pip install tiktoken")

try:
    import faiss
except Exception:
    raise SystemExit("Install faiss-cpu: pip install faiss-cpu")

try:
    from dotenv import load_dotenv
    DOTENV = True
except Exception:
    DOTENV = False

import numpy as np
from tqdm import tqdm

LOG = logging.getLogger("generate_blog")
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

DEFAULT_INDEX_PATH = "rag_sections.faiss"
DEFAULT_META_PATH = "rag_sections_meta.jsonl"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIM_MAP = {"text-embedding-3-large": 3072, "text-embedding-3-small": 1536}
EMBEDDING_DIM = EMBEDDING_DIM_MAP.get(EMBEDDING_MODEL)
ENCODING_NAME = "cl100k_base"

DEFAULT_CHAT_MODEL = "gpt-4o-mini"
DEFAULT_TEMPERATURE = 0.30
DEFAULT_MAX_TOKENS = 1500  # Increased for better content
RETRIEVE_TOP_K = 12  # Increased for better context
RETRIEVE_MULT = 8  # multiply to get more candidates
SUMMARIZE_MAX_TOKENS = 500  # Increased for better summaries
API_RETRY = 5  # Increased retry attempts
API_BACKOFF_BASE = 1.0
MIN_WORD_COUNT = 1500  # Increased minimum word count
ENHANCED_CONTENT_GENERATION = True  # Enable enhanced content strategies
CONTENT_QUALITY_THRESHOLD = 0.8  # Quality threshold for content validation
DIVERSITY_WEIGHT = 0.3  # Weight for content diversity
RELEVANCE_WEIGHT = 0.7  # Weight for content relevance

RULES_BLOCK = r"""
๐ป ููุงูู ุชููุฏ ูุญุชูุง (ุงูุฒุงู โ ุญุชูุง ุฑุนุงุช ุดูุฏ):

๐ ููุงูู ุนููุงู H1:
- ุญุชูุง ุดุงูู ฺฉููู ฺฉูุฏ ุงุตู ุจุงุดุฏ (ุงุฒ ูุดุชูุงุช ุขู ูุจุงุดุฏ) ู ุฏููุง ุฏุฑ ุงุจุชุฏุง ุนููุงู ุขูุฑุฏู ุดูุฏ
- ุฏุฑ ุงุฏุงูู ฺฉ ุฌููู ุชุฑุบุจ ฺฉููุฏู ุดุงูู ุงุนุฏุงุฏุ ุชุฑูุ ุตูุฑ ุชุง ุตุฏุ ฺฏุงู ุจู ฺฏุงูุ ููููู ฺฉุงุฑ ู ... ุขูุฑุฏู ุดูุฏ
- ูุซุงู: "ุฌุฑุงุญ ุจู ฺุณุชุ + ุตูุฑ ุชุง ุตุฏ ุนูู ุจู ุจู ููุฑุงู ููููู ฺฉุงุฑ"
- ุทูู ุนููุงู ุจุดุชุฑ ุงุฒ 40 ุญุฑู ูุจุงุดุฏ

๐ ููุงูู ูุญุชูุง ุงุตู:
- ุงุจุชุฏุง ูุชู ุงุตู ฺฉ ูพุงุฑุงฺฏุฑุงู 3 ุงู 4 ุฎุท ุจุงุดุฏ ฺฉู ุฏููุง ุจุง ุฎูุฏ ฺฉููู ฺฉูุฏ ุดุฑูุน ุดูุฏ
- ุชูุงู ุนููุงู ูุง ุจุง ุชฺฏ H2 ููุดุชู ุดููุฏ ูฺฏุฑ ุฏุฑ ุดุฑุงุท ฺฉู ุจู ูุญุงุธ ูุนูุง ฺฉ ุชุชุฑ ู ุชูุงูุฏ ุฒุฑ ูุฌููุนู ุชุชุฑ ุฏฺฏุฑ ุจุงุดุฏ
- ุจุฑุง ูุซุงู: (ุชุชุฑ h2: ุฏุฑุฏ ุนูู ุจู) ุงฺฏุฑ ุชุชุฑ ุจุนุฏ "ุจุฑุฑุณ ุฑูุด ูุง ฺฉุงูุด ุฏุฑุฏ ุนูู ุจู" ุจุงุดุฏ ู ุชูุงู ุงุฒ H3 ุงุณุชูุงุฏู ฺฉุฑุฏ
- ุงฺฏุฑ ุงุฑุชุจุงุท ูุนูุง ู ุญุงูุช ุฒุฑ ูุฌููุนู ูุจุงุดุฏุ ุชูุงู ุชุชุฑ ูุง H2 ุจุงุดูุฏ

๐ ููุงูู ูพุงุฑุงฺฏุฑุงูโูุง:
- ูุจุงุฏ ูพุงุฑุงฺฏุฑุงู ุจุดุชุฑ ุงุฒ 3 ุงู 4 ุฎุท ุฏุงุดุชู ุจุงุดุฏ
- ุจุงุฏ ุจู ูพุงุฑุงฺฏุฑุงู ุจุนุฏ ุฑูุช ฺูู ุทููุงู ุดุฏู ุจุงุนุซ ุจ ุชูุฌู ฺฉุงุฑุจุฑ ุจู ูุชู ู ุดูุฏ
- ูุฑ ูพุงุฑุงฺฏุฑุงู ุจุงุฏ ฺฉ ุงุฏู ุงุตู ุฏุงุดุชู ุจุงุดุฏ

๐ ููุงูู ุฌุฏูู:
- ุญุชูุง ุฏุฑ ููุงูู ุงุฒ ฺฉ ุฌุฏูู ูุชูุงุณุจ ุจุง ุชู ุฑูฺฏ ูุจ ุณุงุช ุงุณุชูุงุฏู ุดูุฏ
- ูููุช ููุงุณุจ ุนูููุง ุฏุฑ ุณุงุช ูุง ูุงุฑุณ IRANSansWeb ุงุณุชูุงุฏู ู ุดูุฏ
- ุฌุฏูู ุจุงุฏ ุงุทูุงุนุงุช ููุฏ ู ููุงุณูโุง ุงุฑุงุฆู ุฏูุฏ

๐ ููุงูู ุชูุฒุน ฺฉููู ฺฉูุฏ (ุจุณุงุฑ ููู):
- ุจุงุฏ ูุชู ุจุง ฺฉ ูพุฑุงฺฉูุฏฺฏ ุจุณุงุฑ ุทุจุน ู ูุฑูุงู ุงุฒ ฺฉููู ฺฉูุฏ ููุฑุงู ุจุงุดุฏ
- ฺฉููู ฺฉูุฏ ุฏุฑ ูุฑ ูพุงุฑุงฺฏุฑุงู ฺฉ ุจุงุฑ ุจุงุฏ (ูู ุฎู ูพุดุช ุณุฑ ูู ู ูุตููุน)
- ูู ุฎู ุฏูุฑ ู ุจุฏูู ุชฺฉุฑุงุฑ ุจุงุดุฏ
- ุงุฒ ฺฉููุงุช ูุฑุชุจุท ู ููโูุนู ูุฒ ุงุณุชูุงุฏู ฺฉู

๐ ููุงูู ูุญุชูุง ู ูุญู:
- ฺฉุงุฑุจุฑ ูุงุฒ ุจู ูุญุชูุง ุงูุณุงูุ ุจุง ุงุญุณุงุณุ ููุฑุงู ุจุง ูุซุงู ุฏุงุฑุฏ
- ูุญุชูุง ุฑุง ุทูุฑ ุชูู ฺฉู ฺฉู ฺฉุงุฑุจุฑ ุงุญุณุงุณ ฺฉูุฏ ูุงุฒ ุงู ุจุฑุขูุฑุฏู ู ุดูุฏ
- ุงุจูุงู ู ุณูุงูุงุช ุงู ุจุง ุจุงู ุฑูุงู ู ุณุงุฏู ู ุจุฏูู ูพฺุฏฺฏ ูพุงุณุฎ ุฏุงุฏู ู ุดูุฏ
- ูุฎุงุทุจ: ูพุฒุดฺฉุงูุ ูฺฉูุงุ ฺฉููฺฉ ูุง ุฒุจุงุ ฺฉุงุฑุฎุงูู ูุง ู ููู ุงูุฑุงุฏ ฺฉู ูุงุฒ ุจู ุชูุณุนู ฺฉุณุจ ู ฺฉุงุฑ ุฏุงุฑูุฏ
- ูุฏู: ุชุฑุบุจ ฺฉุงุฑุจุฑ ุจู ุงูุฌุงู ุงู ุฎุฏูุงุช (ูู ุจุงู ุนูู ู ุณูฺฏู)
- ูุญุชูุง ุจุงุฏ ฺฉุงููุง ุงูุณุงู ู ฺฉู ุฏูุณุชุงูู ุจุงุดุฏ
- ุญุชูุง ุชูุถุญุงุช ููุฑุงู ูุซุงู ุจุงุดุฏ ู ูุชู ุฑูุงู ุจุงุดุฏ

        ๐ ููุงูู ูฺฏุงุฑุด (ุงูุฒุงู):
        - (ูู ุ ุชู) ุจู ฺฉุงูุง ู ฺฉููู ูุจู ู ุจุนุฏ ุจุงุฏ ูุงุตูู ูุฌูุฏ ุฏุงุดุชู ุจุงุดุฏ
        - ูุนู ูุง ูุงุตูู ููุงุณุจ ุฒุจุงู ูุงุฑุณ ุฑุง ุฏุงุดุชู ุจุงุดูุฏ: ู ุดูุฏ ุ ู ุชูุงูุฏ ุ ุฎูุงูุฏู ุงู ุ ู ุชูุงููุฏ ุ ูุฏุงุดุชู ุงุณุช
        - ูุงุตูู ุญุฑูู ูุงุฑุณ ุจู ุตูุฑุช ุงุณุชุงูุฏุงุฑุฏ: ุฌุง ุจู ุฌุง ุ ุทุฑุงุญ ุณุงุช ุ ุณุฆู ุณุงุช ุ ูุฏ ูุธุฑ ุ ูพุงุฑู ููุช ุ ูุงุจู ูุจูู ุ ุฌุณุช ู ุฌู
        - ูุงุตูู ุจู ฺฉููุงุช ุจุงุฏ ุจู ุงู ุดฺฉู ุฑุนุงุช ุดููุฏ: ุฑุงู ูุง (ุฏุฑุณุช) ุ ุฑุงูฺฉุงุฑ ูุง (ุฏุฑุณุช) ุ ูุจุณุงุช ูุง (ุฏุฑุณุช) - ูู ุฑุงููุง ุ ุฑุงูฺฉุงุฑูุง
        - ุงุณุชูุงุฏู ูุฏุงูู ู ูพุฑ ุชฺฉุฑุงุฑ ุงุฒ "ุชุฑ" ุฑุง ฺฉู ู ุฏุฑ ุญุฏ ุทุจุน ุงูุฌุงู ุจุฏู
        - ูุงุฒ ูุณุช ูพุดุช ุณุฑ ููุดุชู ุดูุฏ: ุณุฑุน ุชุฑ ุ ููู ุชุฑ ุ ฺฉุงุฑุจุฑุฏ ุชุฑ (ูุชู ุฑุง ูุตููุน ู ฺฉูุฏ)
        - ูุชู ุจุงุฏ ุงูุณุงู ู ุฏุฑ ุจุนุถ ูุณูุช ูุง ูุชู ูุฌุงู ู ุฏุฑ ุจุนุถ ูุณูุช ูุง ุฏูุณุชุงูู ุจุงุดุฏ

๐ ููุงูู ุนููุงู ุณุฆู:
- ุญุชูุง ุดุงูู ฺฉููู ฺฉูุฏ ุงุตู + ฺฉ ูุชู ุชุฑุบุจ ฺฉููุฏู ู ุฌุฐุงุจ ุจุงุดุฏ

๐ ููุงูู ุทูู ูุญุชูุง:
- ฺฉู ููุงูู ููุง ุญุฏุงูู 1000 ฺฉููู ุจุงุดุฏ
- ูุญุชูุง ุจุงุฏ ุฌุงูุน ู ฺฉุงูู ุจุงุดุฏ
"""

EXAMPLES_BLOCK = r"""
ูุซุงูโูุง ุนูู (ููููู ูุฑูุฏ -> ุฎุฑูุฌ):

        ๐ ูุซุงูโูุง ูฺฏุงุฑุด:
- ฺฉุงูุง: ูุงุฏุฑุณุช -> "ููุุชู"; ุตุญุญ -> "ูู ุ ุชู"
- ูุนู ูพูุณุชู: ูุงุฏุฑุณุช -> "ูุดูุฏ" ุง "ูโุดูุฏ"; ุตุญุญ -> "ู ุดูุฏ"
- ูุงุตูู ฺฉููุงุช: ูุงุฏุฑุณุช -> "ุทุฑุงุญุณุงุช" ุง "ุฌุงุจูุฌุง"; ุตุญุญ -> "ุทุฑุงุญ ุณุงุช" ุ "ุฌุง ุจู ุฌุง"
        - ูุงุตูู ฺฉููุงุช ูุฑฺฉุจ: ูุงุฏุฑุณุช -> "ุฑุงููุง" ุ "ุฑุงูฺฉุงุฑูุง"; ุตุญุญ -> "ุฑุงู ูุง" ุ "ุฑุงูฺฉุงุฑ ูุง"
        - ูุงุตูู ฺฉููุงุช ูุฑฺฉุจ: ูุงุฏุฑุณุช -> "ูุจุณุงุชูุง"; ุตุญุญ -> "ูุจุณุงุช ูุง"
        - ุงุณุชูุงุฏู ุงุฒ "ุชุฑ": ูุงุฏุฑุณุช -> "ุณุฑุน ุชุฑ ุ ููู ุชุฑ ุ ฺฉุงุฑุจุฑุฏ ุชุฑ"; ุตุญุญ -> "ุณุฑุน ุ ููู ุ ฺฉุงุฑุจุฑุฏ"

๐ ูุซุงูโูุง ุนููุงู H1:
- ฺฉูุฏูุงฺู "ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ" -> "ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ + ุตูุฑ ุชุง ุตุฏ ุฑุงูููุง ฺฉุงูู ุจุง ููููู ฺฉุงุฑูุง"
- ฺฉูุฏูุงฺู "ุณุฆู ุณุงุช" -> "ุณุฆู ุณุงุช + 10 ุฑูุด ุทูุง ุงูุฒุงุด ุฑุชุจู ุฏุฑ ฺฏูฺฏู"
- ฺฉูุฏูุงฺู "ุฌุฑุงุญ ุจู" -> "ุฌุฑุงุญ ุจู + ุตูุฑ ุชุง ุตุฏ ุนูู ุจู ุจู ููุฑุงู ููููู ฺฉุงุฑ"

๐ ูุซุงูโูุง ูพุงุฑุงฺฏุฑุงู ุขุบุงุฒู:
- ุดุฑูุน ุจุง ฺฉููู ฺฉูุฏ: "ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ฺฉ ุงุฒ ูููโุชุฑู ุงุจุฒุงุฑูุง ุจุงุฒุงุฑุงุจ ุฏุฌุชุงู ุจุฑุง ฺฉููฺฉโูุง ู ูุทุจโูุง ูพุฒุดฺฉ ุงุณุช. ุฏุฑ ุฏูุง ุงูุฑูุฒ ฺฉู ุจูุงุฑุงู ูุจู ุงุฒ ูุฑุงุฌุนู ุจู ูพุฒุดฺฉุ ุงุจุชุฏุง ุฏุฑ ุงูุชุฑูุช ุฌุณุชโูุฌู ูโฺฉููุฏุ ุฏุงุดุชู ฺฉ ูุจโุณุงุช ุญุฑููโุง ู ุจูููโุดุฏู ุจุฑุง ููุชูุฑูุง ุฌุณุชโูุฌูุ ูโุชูุงูุฏ ุชูุงูุช ุจุฒุฑฺฏ ุฏุฑ ุฌุฐุจ ุจูุงุฑุงู ุฌุฏุฏ ุงุฌุงุฏ ฺฉูุฏ."

๐ ูุซุงูโูุง ุฌุฏูู:
<table style="font-family: IRANSansWeb; border-collapse: collapse; width: 100%; margin: 20px 0;">
<tr style="background-color: #f2f2f2;">
<th style="border: 1px solid #ddd; padding: 12px; text-align: right;">ูุฒุงุง</th>
<th style="border: 1px solid #ddd; padding: 12px; text-align: right;">ูุจู ุงุฒ ุทุฑุงุญ ุณุงุช</th>
<th style="border: 1px solid #ddd; padding: 12px; text-align: right;">ุจุนุฏ ุงุฒ ุทุฑุงุญ ุณุงุช</th>
</tr>
<tr>
<td style="border: 1px solid #ddd; padding: 12px;">ุชุนุฏุงุฏ ุจูุงุฑุงู</td>
<td style="border: 1px solid #ddd; padding: 12px;">10 ููุฑ ุฏุฑ ูุงู</td>
<td style="border: 1px solid #ddd; padding: 12px;">50 ููุฑ ุฏุฑ ูุงู</td>
</tr>
</table>

๐ ูุซุงูโูุง ุชูุฒุน ฺฉููู ฺฉูุฏ:
- ูพุงุฑุงฺฏุฑุงู 1: "ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ุงูุฑูุฒู ุถุฑูุฑ ุงุณุช..."
- ูพุงุฑุงฺฏุฑุงู 2: "ุจุฑุง ุดุฑูุน ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉุ ุงุจุชุฏุง ุจุงุฏ..."
- ูพุงุฑุงฺฏุฑุงู 3: "ูุฒูู ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ุจุณุชฺฏ ุจู..."

๐ ูุซุงูโูุง ุนููุงู ุณุฆู:
- "ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ: ุงูุฒุงุด ุจูุงุฑุงู ุจุง ูุจโุณุงุช ุญุฑููโุง"
- "ุณุฆู ุณุงุช: ุฑุงูููุง ฺฉุงูู ุงูุฒุงุด ุฑุชุจู ุฏุฑ ฺฏูฺฏู"
- "ุฌุฑุงุญ ุจู: ุตูุฑ ุชุง ุตุฏ ุนูู ุจู ุจุง ุจูุชุฑู ุฌุฑุงุญุงู"

๐ ูุซุงูโูุง ูุญู:
- ูุฌุงู: "ุชุตูุฑ ฺฉูุฏ ฺฉุณุจโูฺฉุงุฑุชุงู ุฑููู ุจฺฏุฑุฏ! ุจุง ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ุญุฑููโุงุ ูโุชูุงูุฏ..."
- ุฏูุณุชุงูู: "ุจุงุฏ ุจุง ูู ุจุฑุฑุณ ฺฉูู ฺุทูุฑ ุงู ฺฉุงุฑ ุฑุง ุงูุฌุงู ุฏูุฏ. ุฏุฑ ุงุฏุงููุ ูุฑุงุญู ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ุฑุง..."
- ุชุฑุบุจโฺฉููุฏู: "ุงฺฏุฑ ูโุฎูุงูุฏ ุจูุงุฑุงู ุจุดุชุฑ ุฌุฐุจ ฺฉูุฏุ ุทุฑุงุญ ุณุงุช ูพุฒุดฺฉ ุงููู ูุฏู ุงุณุช..."
"""

def load_env(env_path: Optional[Path]):
    if env_path:
        if DOTENV:
            load_dotenv(dotenv_path=str(env_path))
            LOG.info("Loaded .env from %s", env_path)
        else:
            with env_path.open(encoding="utf-8") as f:
                for ln in f:
                    ln = ln.strip()
                    if not ln or ln.startswith("#"):
                        continue
                    if "=" in ln:
                        k, v = ln.split("=", 1)
                        os.environ.setdefault(k.strip(), v.strip().strip('"').strip("'"))
            LOG.info("Loaded .env (simple parser) from %s", env_path)

def get_openai_client() -> OpenAI:
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        raise RuntimeError("OPENAI_API_KEY not found in environment. Provide via .env or env var.")
    return OpenAI(api_key=key)

_enc = tiktoken.get_encoding(ENCODING_NAME)

def embed_text(client: OpenAI, texts: List[str], model: str = EMBEDDING_MODEL) -> List[List[float]]:
    out = []
    batch_size = 16
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        last_exc = None
        for attempt in range(API_RETRY):
            try:
                resp = client.embeddings.create(model=model, input=batch)
                for item in resp.data:
                    out.append(list(item.embedding))
                break
            except Exception as e:
                last_exc = e
                wait = API_BACKOFF_BASE * (2 ** attempt)
                LOG.warning("Embedding attempt %d failed: %s โ retrying in %.1fs", attempt + 1, e, wait)
                time.sleep(wait)
        else:
            raise RuntimeError(f"Failed embedding batch after retries: {last_exc}")
    return out

def embed_query(client: OpenAI, query: str, model: str = EMBEDDING_MODEL) -> List[float]:
    return embed_text(client, [query], model=model)[0]

def load_meta_jsonl(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        raise FileNotFoundError(f"Metadata file not found: {path}")
    meta = []
    with path.open("r", encoding="utf-8") as f:
        for ln in f:
            ln = ln.strip()
            if not ln:
                continue
            meta.append(json.loads(ln))
    return meta

def load_faiss_index(path: Path) -> faiss.Index:
    if not path.exists():
        raise FileNotFoundError(f"FAISS index file not found: {path}")
    return faiss.read_index(str(path))

def retrieve_top_k(client: OpenAI, index: faiss.Index, meta: List[Dict[str, Any]],
                   query: str, top_k: int = RETRIEVE_TOP_K) -> List[Tuple[Dict[str,Any], float]]:
    """
    Retrieve top_k chunks globally for the query.
    Searches for N = top_k * RETRIEVE_MULT, keeps unique by id, ordered by score desc.
    """
    vec = np.array([embed_query(client, query)], dtype="float32")
    faiss.normalize_L2(vec)
    N = top_k * RETRIEVE_MULT
    max_N = min(len(meta), top_k * 20)
    attempts = 0
    while attempts < 3:
        D, I = index.search(vec, N)
        candidates = []
        for score, idx in zip(D[0], I[0]):
            if idx < 0 or idx >= len(meta):
                continue
            m = meta[idx]
            candidates.append((m, float(score)))
        # keep unique by chunk id and preserve order by score
        seen = set()
        filtered = []
        for m, s in candidates:
            cid = m.get("id")
            if cid in seen:
                continue
            seen.add(cid)
            filtered.append((m, s))
        results = filtered[:top_k]
        if len(results) >= top_k:
            return results
        # increase N
        attempts += 1
        N = min(max_N, N * 2)
    LOG.warning("Insufficient retrieval results for query '%s'; returning %d", query, len(results))
    return results

def summarize_section_for_continuation(client: OpenAI, text: str) -> Dict[str, Any]:
    """
    Ask the model to produce a compact JSON brief.
    Improved prompt for strict JSON output.
    """
    user_prompt = (
        "ุฎูุงุตูโุง ฺฉูุชุงู ู ุณุงุฎุชุงุฑููุฏ ุจุฑุง ุงุฏุงููู ูุชู ุชููุฏ ฺฉู. ุฎุฑูุฌ ููุท ฺฉ JSON ูุนุชุจุฑ ุจุฏู ุจุง ููุฏูุง ุฒุฑ:\n"
        "title (ุนููุงู ุจุฎุด)ุ last_sentence (ุขุฎุฑู ุฌููู ุจุฎุด)ุ main_points (ูุณุช ุงุฒ ุญุฏุงฺฉุซุฑ 6 ูฺฉุชู ฺฉูุฏ)ุ "
        "tone (ฺฉ ฺฉููู: ูุซู 'ุชุฑุบุจโฺฉููุฏู' ุง 'ุงุทูุงุน')ุ suggested_next_headings (ุขุฑุงูโุง ุงุฒ 3 ุชุชุฑ ูพุดููุงุฏ ุจุฑุง ุจุฎุด ุจุนุฏ).\n\n"
        f"ูุชู:\n{text}\n\n"
        "ูุฑูุช ุฎุฑูุฌ: JSON ุฎุงูุต. ูฺ ูุชู ุงุถุงู ูููุณ. ูุทูุฆู ุดู ุชูุงู ุฑุดุชูโูุง ุจู ุฏุฑุณุช ุจุณุชู ุดููุฏ ู JSON ูุนุชุจุฑ ุจุงุดุฏ."
    )
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=DEFAULT_CHAT_MODEL,
                messages=[{"role": "system", "content": "ุดูุง ฺฉ ุฎูุงุตูโุณุงุฒ ูุณุชุฏ ฺฉู ููุท JSON ูุนุชุจุฑ ุชููุฏ ูโฺฉูุฏ. ูฺ ูุชู ุฏฺฏุฑ ูููุณ."},
                          {"role": "user", "content": user_prompt}],
                temperature=0.0,
                max_tokens=SUMMARIZE_MAX_TOKENS,
            )
            out = resp.choices[0].message.content.strip()
            # remove code fences if present
            out = re.sub(r"^```json\s*|\s*```$", "", out, flags=re.I)
            # attempt to fix unterminated string by adding " at end if needed
            if out[-1] != '}':
                out += '" }'
            m = re.search(r"(\{.*\})", out, flags=re.S)
            json_text = m.group(1) if m else out
            parsed = json.loads(json_text)
            return parsed
        except Exception as e:
            last_exc = e
            LOG.warning("Summarization attempt %d failed: %s", attempt + 1, e)
            time.sleep(API_BACKOFF_BASE * (2 ** attempt))
    LOG.warning("Summarization failed: %s. Falling back.", last_exc)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    last_sentence = lines[-1] if lines else ""
    brief = {
        "title": "",
        "last_sentence": last_sentence[:200],
        "main_points": lines[:6],  # Increased to 6
        "tone": "ุชุฑุบุจโฺฉููุฏู",
        "suggested_next_headings": []
    }
    return brief

def generate_structure(client: OpenAI, index: faiss.Index, meta: List[Dict[str,Any]],
                       keyword: str, perfect_html_reference: Optional[str], model: str = DEFAULT_CHAT_MODEL,
                       temperature: float = DEFAULT_TEMPERATURE, max_tokens: int = DEFAULT_MAX_TOKENS) -> Dict[str, Any]:
    LOG.info("Generating blog structure for keyword=%s", keyword)
    
    # Enhanced query with keyword variations for better RAG retrieval
    keyword_variations = generate_keyword_variations(keyword)
    enhanced_query = f"{keyword} {' '.join(keyword_variations[:3])}"  # Use top 3 variations
    
    retrieved = retrieve_top_k(client, index, meta, enhanced_query, top_k=RETRIEVE_TOP_K)
    
    # Filter and prioritize relevant content based on keyword similarity
    relevant_chunks = []
    for m, score in retrieved:
        text = m.get("text", "").lower()
        title = m.get("section_title", "").lower()
        
        # Check if content is relevant to the keyword
        keyword_lower = keyword.lower()
        is_relevant = (
            keyword_lower in text or 
            keyword_lower in title or
            any(variation.lower() in text for variation in keyword_variations[:3]) or
            any(variation.lower() in title for variation in keyword_variations[:3])
        )
        
        if is_relevant or score > 0.7:  # Include high-scoring content even if not directly matching
            relevant_chunks.append((m, score))
    
    # If we don't have enough relevant content, use the original retrieved content
    if len(relevant_chunks) < 3:
        relevant_chunks = retrieved
    
    context_parts = []
    for m, score in relevant_chunks:
        src = m.get("source_file", "")
        ci = m.get("chunk_index", 0)
        txt = m.get("text", "")
        context_parts.append(f"--- ููุจุน: [{src}] (chunk {ci}, score={score:.4f})\n{txt}")
    context_block = "\n\n".join(context_parts) if context_parts else ""

    perfect_ref_block = ""
    if perfect_html_reference:
        perfect_ref_block = (
            "ุงูฺฏู ูุฑุฌุน (ุจุฑุง ุณุงุฎุชุงุฑ/ุฌุฏุงุณุงุฒ ูพุงุฑุงฺฏุฑุงูโูุง/ุงุณุชุงูโูุง/ูุฌูุฏ ุฌุฏูู ู CTA):\n"
            + perfect_html_reference[:4000] + "\n\n"
        )

    prompt = (
        f"{RULES_BLOCK}\n"
        f"{EXAMPLES_BLOCK}\n\n"
        f"๐ฏ ฺฉููู ฺฉูุฏ ุงุตู: {keyword}\n"
        f"๐ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู (ููุท ุงุฒ ุงู ููุงุจุน ุจุฑุง ุงููุงูโฺฏุฑ ู ุงุณุชูุงุฏ ุงุณุชูุงุฏู ฺฉู):\n{context_block}\n\n"
        f"{perfect_ref_block}"
        "๐ ูุธูู: ุณุงุฎุชุงุฑ ููุงูู ุฑุง ุจู ุตูุฑุช JSON ุชููุฏ ฺฉู:\n"
        "{\n"
        f'  "h1_title": "ุนููุงู H1 (ุดุงูู ฺฉููู ฺฉูุฏ ุงุตู "{keyword}" + ุนุจุงุฑุช ุชุฑุบุจ ฺฉููุฏู)",\n'
        f'  "seo_title": "ุนููุงู ุณุฆู (ฺฉููู ฺฉูุฏ ุงุตู "{keyword}" + ูุชู ุฌุฐุงุจ)",\n'
        '  "sections": [\n'
        '    {"title": "ุชุชุฑ ุจุฎุด", "level": 2, "needs_table": false, "description": "ุชูุถุญ ฺฉูุชุงู ูุญุชูุง"},\n'
        '    {"title": "ุชุชุฑ ุจุฎุด", "level": 2, "needs_table": true, "description": "ุชูุถุญ ฺฉูุชุงู ูุญุชูุง"}\n'
        '  ]\n'
        "}\n\n"
        "โ๏ธ ููุงูู ููู ุณุงุฎุชุงุฑ:\n"
        f"- ุญุชูุง ฺฉููู ฺฉูุฏ ุงุตู '{keyword}' ุฑุง ุฏุฑ ุนููุงู H1 ู ุณุฆู ุงุณุชูุงุฏู ฺฉู\n"
        "- ุชูุงู ุจุฎุดโูุง ุจุงุฏ ูุฑุชุจุท ุจุง ููุถูุน '{keyword}' ุจุงุดูุฏ\n"
        "- ุงุฒ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู ุจุฑุง ุงููุงูโฺฏุฑ ุงุณุชูุงุฏู ฺฉูุ ูู ุงุฒ ุฏุงูุด ุนููู\n"
        "- ุญุฏุงูู 6 ุจุฎุด ุฏุงุดุชู ุจุงุด (ุจู ุฌุฒ ููุฏูู)\n"
        "- ุญุฏุงูู 2 ุจุฎุด ููุงุณุจ ุจุฑุง ุฌุฏูู ูพุดููุงุฏ ฺฉู (ููุงุณูุ ูุฒุงุงุ ูุฑุงุญูุ ุขูุงุฑ)\n"
        "- ุนูุงูู ุจุงุฏ ุฌุฐุงุจ ู ุชุฑุบุจโฺฉููุฏู ุจุงุดูุฏ\n"
        "- ุงุฒ ุงุนุฏุงุฏ ู ฺฉููุงุช ูุฏุฑุชููุฏ ุงุณุชูุงุฏู ฺฉู (10 ุฑูุดุ 5 ูุฒุชุ ุตูุฑ ุชุง ุตุฏ)\n"
        "- ูุฑ ุจุฎุด ุจุงุฏ ฺฉ ูุฏู ูุดุฎุต ุฏุงุดุชู ุจุงุดุฏ\n"
        "- ุฎุฑูุฌ ููุท JSON ูุนุชุจุฑ ุจุงุดุฏ"
    )

    generated = None
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ุดูุง ฺฉ ูุชุฎุตุต ุจุฑูุงููโุฑุฒ ูุญุชูุง ู ุณุงุฎุชุงุฑ ููุงูู ูุณุชุฏ. ุชุฎุตุต ุดูุง ุฏุฑ ุงุฌุงุฏ ุณุงุฎุชุงุฑูุง ุจููู ู ุฌุฐุงุจ ุจุฑุง ููุงูุงุช ูุงุฑุณ ุงุณุช ฺฉู ูู ุจุฑุง ฺฉุงุฑุจุฑุงู ุฌุฐุงุจ ุจุงุดุฏ ู ูู ุจุฑุง ููุชูุฑูุง ุฌุณุชโูุฌู ุจููู ุจุงุดุฏ. ุดูุง ููุงูู SEO ู ุณุงุฎุชุงุฑ ูุญุชูุง ุฑุง ุจู ุทูุฑ ฺฉุงูู ุฑุนุงุช ูโฺฉูุฏ."},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            out = resp.choices[0].message.content.strip()
            out = re.sub(r"^```json\s*|\s*```$", "", out, flags=re.I)
            parsed = json.loads(out)
            return parsed
        except Exception as e:
            last_exc = e
            wait = API_BACKOFF_BASE * (2 ** attempt)
            LOG.warning("Structure gen attempt %d failed: %s - retrying in %.1fs", attempt + 1, e, wait)
            time.sleep(wait)
    raise RuntimeError(f"Failed to generate structure after retries: {last_exc}")

def build_section_prompt(keyword: str, section_title: str, section_level: int, section_idx: int, retrieved_chunks: List[Tuple[Dict[str,Any], float]],
                         prev_brief: Optional[Dict[str,Any]], perfect_html_reference: Optional[str], examples_block: str) -> str:
    context_parts = []
    for m, score in retrieved_chunks:
        src = m.get("source_file", "")
        ci = m.get("chunk_index", 0)
        txt = m.get("text", "")
        context_parts.append(f"--- ููุจุน: [{src}] (chunk {ci}, score={score:.4f})\n{txt}")
    context_block = "\n\n".join(context_parts) if context_parts else ""

    prev_block = ""
    if prev_brief:
        prev_block = (
            "ุฎูุงุตูู ุจุฎุด ูุจู (ุจุฑุง ุญูุธ ูพูุณุชฺฏุ ุงุฒ ุขู ุงุณุชูุงุฏู ฺฉูุ ุงุฏุงูู ุจุฏู ุงุฒ ุขุฎุฑู ุฌูููุ ูุญู ุฑุง ุญูุธ ฺฉู ู ุชฺฉุฑุงุฑ ูุญุชูุง ุฏูู ุจุฎุด ูุจู ูฺฉู):\n"
            + json.dumps(prev_brief, ensure_ascii=False, indent=2)
            + "\n\n"
        )

    perfect_ref_block = ""
    if perfect_html_reference:
        perfect_ref_block = (
            "ุงูฺฏู ูุฑุฌุน:\n"
            + perfect_html_reference[:4000] + "\n\n"
        )

    if section_idx == 0:
        # Intro section: H1 + intro para
        task = (
            "๐ ูุธูู ุจุฎุด ููุฏูู:\n"
            " - <h1>ุนููุงู H1</h1> ุฑุง ุชููุฏ ฺฉู (ุดุงูู ฺฉููู ฺฉูุฏ + ุนุจุงุฑุช ุชุฑุบุจ ฺฉููุฏู)\n"
            " - ูพุงุฑุงฺฏุฑุงู ุขุบุงุฒู 3-4 ุฎุท ฺฉู ุฏููุง ุจุง ฺฉููู ฺฉูุฏ ุดุฑูุน ุดูุฏ\n"
            " - ูุญุชูุง ุจุงุฏ ูุฌุงู ู ุชุฑุบุจโฺฉููุฏู ุจุงุดุฏ\n"
            " - ุงุฒ ุงุนุฏุงุฏ ู ุขูุงุฑ ุงุณุชูุงุฏู ฺฉู (ูุซู: 80% ุงุฒ ุจูุงุฑุงูุ 5 ุจุฑุงุจุฑ ุงูุฒุงุด)\n"
            " - ุฌุฏูู ุงุถุงูู ูฺฉู"
        )
    else:
        # Regular section
        needs_table = "needs_table" in section_title.lower() or any(word in section_title.lower() for word in ["ููุงุณู", "ูุฒุงุง", "ูุฑุงุญู", "ุขูุงุฑ", "ุฌุฏูู", "ูุณุช"])
        table_instruction = ""
        if needs_table:
            table_instruction = (
                "\n - ฺฉ ุฌุฏูู ููุฏ ู ูุฑุชุจุท ุงุถุงูู ฺฉู ุจุง ุงุณุชุงู:\n"
                '<table style="font-family: IRANSansWeb; border-collapse: collapse; width: 100%; margin: 20px 0;">\n'
                '<tr style="background-color: #f2f2f2;">\n'
                '<th style="border: 1px solid #ddd; padding: 12px; text-align: right;">ุณุชูู 1</th>\n'
                '<th style="border: 1px solid #ddd; padding: 12px; text-align: right;">ุณุชูู 2</th>\n'
                '</tr>\n'
                '<tr>\n'
                '<td style="border: 1px solid #ddd; padding: 12px;">ุฏุงุฏู 1</td>\n'
                '<td style="border: 1px solid #ddd; padding: 12px;">ุฏุงุฏู 2</td>\n'
                '</tr>\n'
                '</table>'
            )
        
        task = (
            f"๐ ูุธูู ุจุฎุด: {section_title}\n"
            f" - <h{section_level}>{section_title}</h{section_level}> ุฑุง ุชููุฏ ฺฉู\n"
            " - ูุญุชูุง ุฌุงูุน ู ููุตู ุจุฑุง ุงู ุจุฎุด ุจููุณ\n"
            " - ุงุฒ ูุซุงูโูุง ุนูู ู ฺฉุงุฑุจุฑุฏ ุงุณุชูุงุฏู ฺฉู\n"
            " - ูุญู ุฏูุณุชุงูู ู ูุงุจู ููู ุฏุงุดุชู ุจุงุด\n"
            " - ูุฑ ูพุงุฑุงฺฏุฑุงู 3-4 ุฎุท ุจุงุดุฏ\n"
            f"{table_instruction}"
        )

    prompt = (
        f"{RULES_BLOCK}\n"
        f"{examples_block}\n\n"
        f"{prev_block}"
        f"๐ฏ ุจุฎุด ฺฉููู: {section_title} (ุณุทุญ {section_level})\n"
        f"๐ ุดูุงุฑู ุจุฎุด: {section_idx}\n"
        f"๐ ฺฉููู ฺฉูุฏ ุงุตู: {keyword}\n\n"
        "๐ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู (ุงูุฒุงู - ุญุชูุง ุงุฒ ุงู ููุงุจุน ุงุณุชูุงุฏู ฺฉู):\n"
        f"{context_block}\n\n"
        f"{perfect_ref_block}"
        f"{task}\n\n"
        "โ๏ธ ุฏุณุชูุฑุงูุนููโูุง ุงูุฒุงู:\n"
        f" - ุญุชูุง ฺฉููู ฺฉูุฏ '{keyword}' ุฑุง ุฏุฑ ูุญุชูุง ุงุณุชูุงุฏู ฺฉู\n"
        " - ููุท ุจุฑุง ุงู ุจุฎุด ูุญุชูุง ุชููุฏ ฺฉูุ ุชฺฉุฑุงุฑ ุจุฎุด ูุจู ูฺฉู\n"
        " - ุฌุฑุงู ู ูพูุณุชฺฏ ุจุง ุจุฎุด ูุจู ุฑุง ุญูุธ ฺฉู\n"
        " - ฺฉููู ฺฉูุฏ ุฑุง ุจู ุทูุฑ ุทุจุน ุฏุฑ ูุฑ ูพุงุฑุงฺฏุฑุงู ฺฉ ุจุงุฑ ุงุณุชูุงุฏู ฺฉู\n"
        " - ุงุฒ ฺฉููุงุช ูุฑุชุจุท ู ููโูุนู ูุฒ ุงุณุชูุงุฏู ฺฉู\n"
        " - ูุญุชูุง ุจุงุฏ ุงูุณุงูุ ุจุง ุงุญุณุงุณ ู ููุฑุงู ุจุง ูุซุงู ุจุงุดุฏ\n"
        " - ุญุชูุง ุงุฒ ุงุทูุงุนุงุช ุจุงุฒุงุจโุดุฏู ุจุฑุง ุฏูุช ู ุฌููฺฏุฑ ุงุฒ ุชููู ุงุณุชูุงุฏู ฺฉู\n"
        " - ุงฺฏุฑ ุงุทูุงุนุงุช ฺฉุงู ุฏุฑ ููุงุจุน ูุณุชุ ุงุฒ ุฏุงูุด ุนููู ุงุณุชูุงุฏู ฺฉู ุงูุง ุญุชูุง ฺฉููู ฺฉูุฏ ุฑุง ุฑุนุงุช ฺฉู\n"
        " - ุฎุฑูุฌ ุฑุง ุจู ุตูุฑุช HTML ูุนุชุจุฑ ุจููุณ\n"
        " - ุฏุฑ ุงูุชูุงุ ุงฺฏุฑ ุงุฒ ููุงุจุน ุงุณุชูุงุฏู ฺฉุฑุฏุ <p><strong>ููุงุจุน:</strong> [source_file]</p> ุงุถุงูู ฺฉู\n"
    )
    return prompt

def coherence_edit(client: OpenAI, combined: str, keyword: str, rules_block: str, examples_block: str,
                   model: str = DEFAULT_CHAT_MODEL, temperature: float = DEFAULT_TEMPERATURE,
                   max_tokens: int = DEFAULT_MAX_TOKENS) -> str:
    word_count = count_words(combined)
    expand_note = f"ุงฺฏุฑ ฺฉูุชุฑ ุงุฒ {MIN_WORD_COUNT} ฺฉููู ุงุณุชุ ุจู ุทูุฑ ุทุจุน ฺฏุณุชุฑุด ุจุฏู." if word_count < MIN_WORD_COUNT else ""
    prompt = (
        f"{rules_block}\n"
        f"{examples_block}\n\n"
        "๐ ุจุฑุฑุณ ู ุจูุจูุฏ ููุงูู:\n"
        "ููุงูู HTML ุฒุฑ ุฑุง ุจุฑุง ููุงุฑุฏ ุฒุฑ ุจุฑุฑุณ ู ุจูุจูุฏ ุจุฏู:\n"
        "โ ูพูุณุชฺฏ ู ุฌุฑุงู ุจู ุจุฎุดโูุง\n"
        "โ ุชูุฒุน ุทุจุน ู ูุชุนุงุฏู ฺฉููู ฺฉูุฏ\n"
        "โ ูุญู ฺฉูพุงุฑฺู ู ุงูุณุงู\n"
        "โ ุฑุนุงุช ููุงูู ูฺฏุงุฑุด ูุงุฑุณ\n"
        "โ ฺฉูุช ู ุนูู ูุญุชูุง\n"
        "โ ุงุณุชูุงุฏู ุงุฒ ูุซุงูโูุง ฺฉุงุฑุจุฑุฏ\n"
        "โ ุชุฑุบุจโฺฉููุฏฺฏ ู ุฌุฐุงุจุช\n"
        f"๐ {expand_note}\n\n"
        "๐ฏ ุฎุฑูุฌ ููุง:\n"
        "- HTML ุจูุจูุฏ ุงูุชู ู ุจููู\n"
        "- ูุญุชูุง ุฑูุงู ู ูุงุจู ููู\n"
        "- ุฑุนุงุช ฺฉุงูู ููุงูู ูุงุฑุณ\n"
        "- ุฌุฐุงุจ ู ุชุฑุบุจโฺฉููุฏู\n\n"
        f"๐ ููุงูู ูุนู:\n{combined}"
    )
    generated = None
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ุดูุง ฺฉ ูุฑุงุดฺฏุฑ ุญุฑููโุง ู ูุชุฎุตุต ุจูุจูุฏ ูุญุชูุง ูุงุฑุณ ูุณุชุฏ. ุชุฎุตุต ุดูุง ุฏุฑ ุจูุจูุฏ ฺฉูุชุ ูพูุณุชฺฏุ ู ุฌุฐุงุจุช ููุงูุงุช ูุงุฑุณ ุงุณุช. ุดูุง ููุงูู ูฺฏุงุฑุดุ SEOุ ู ุณุงุฎุชุงุฑ ูุญุชูุง ุฑุง ุจู ุทูุฑ ฺฉุงูู ุฑุนุงุช ูโฺฉูุฏ ู ูุญุชูุง ููุง ุฑุง ุจูููโุณุงุฒ ูโฺฉูุฏ."},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens * 2,  # Allow more for edit
            )
            generated = resp.choices[0].message.content.strip()
            break
        except Exception as e:
            last_exc = e
            wait = API_BACKOFF_BASE * (2 ** attempt)
            LOG.warning("Edit attempt %d failed: %s - retrying in %.1fs", attempt + 1, e, wait)
            time.sleep(wait)
    if generated is None:
        LOG.warning("Coherence edit failed: %s. Returning original.", last_exc)
        return combined
    return normalize_persian_spacing_and_mi(generated)

def generate_blog(client: OpenAI, index: faiss.Index, meta: List[Dict[str,Any]],
                  keyword: str, out_path: Path, perfect_html_path: Optional[Path] = None,
                  model: str = DEFAULT_CHAT_MODEL, temperature: float = DEFAULT_TEMPERATURE,
                  max_tokens: int = DEFAULT_MAX_TOKENS):
    LOG.info("Generating comprehensive blog for keyword=%s", keyword)
    perfect_html_reference = None
    if perfect_html_path and perfect_html_path.exists():
        perfect_html_reference = perfect_html_path.read_text(encoding="utf-8", errors="ignore")

    # Phase 1: Generate comprehensive first section with H1 and 2-3 paragraphs
    LOG.info("Phase 1: Generating comprehensive introduction section...")
    phase1_content, next_section_prompt = generate_phase1_content(
        client, index, meta, keyword, perfect_html_reference, model, temperature, max_tokens
    )
    
    # Clean Phase 1 content
    phase1_content = clean_html_content(phase1_content)
    
    # Phase 2: Generate remaining content based on Phase 1 prompt
    LOG.info("Phase 2: Generating remaining content based on Phase 1 prompt...")
    phase2_content = generate_phase2_content(
        client, index, meta, keyword, next_section_prompt, perfect_html_reference, model, temperature, max_tokens
    )
    
    # Clean Phase 2 content
    phase2_content = clean_html_content(phase2_content)
    
    # Combine both phases
    combined = phase1_content + "\n" + phase2_content
    
    # Phase 3: Final validation and improvement
    LOG.info("Phase 3: Final validation and improvement...")
    
    # Perform advanced content quality check
    quality_metrics = advanced_content_quality_check(combined, keyword)
    LOG.info("Content quality metrics: %s", quality_metrics)
    
    # If quality is below threshold, perform enhanced validation
    if not quality_metrics["is_high_quality"]:
        LOG.info("Content quality below threshold, performing enhanced validation...")
        combined = validate_and_improve_content(client, combined, keyword, model, temperature, max_tokens)
        
        # Re-check quality after improvement
        quality_metrics = advanced_content_quality_check(combined, keyword)
        LOG.info("Post-improvement quality metrics: %s", quality_metrics)
    else:
        LOG.info("Content quality is adequate, performing standard validation...")
        combined = validate_and_improve_content(client, combined, keyword, model, temperature, max_tokens)
    
    # Final coherence edit
    LOG.info("Performing final coherence edit...")
    combined = coherence_edit(client, combined, keyword, RULES_BLOCK, EXAMPLES_BLOCK, model, temperature, max_tokens)

    # Final content enhancement
    if ENHANCED_CONTENT_GENERATION:
        LOG.info("Applying final content enhancements...")
        combined = enhance_content_with_examples(combined, keyword)
        combined = optimize_keyword_distribution(combined, keyword)
        combined = enhance_keyword_distribution(combined, keyword)
        combined = add_engaging_elements(combined)
        combined = normalize_persian_spacing_and_mi(combined)
        
        # Validate keyword adherence
        if not validate_keyword_adherence(combined, keyword):
            LOG.warning("Generated content may not sufficiently adhere to keyword: %s", keyword)

    word_count = count_words(combined)
    LOG.info("Final word count: %d", word_count)
    
    # Extract SEO title from H1
    seo_title = extract_seo_title_from_content(combined, keyword)

    # Wrap in full HTML with SEO title
    full_html = f"""<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <title>{seo_title}</title>
    <!-- Add styles for table if needed -->
</head>
<body dir="rtl">
{combined}
</body>
</html>"""

    combined = normalize_persian_spacing_and_mi(full_html)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(combined, encoding="utf-8")
    LOG.info("Final blog saved to %s (%d words)", out_path, count_words(combined))
    return out_path

def generate_phase1_content(client: OpenAI, index: faiss.Index, meta: List[Dict[str,Any]], 
                           keyword: str, perfect_html_reference: Optional[str], 
                           model: str, temperature: float, max_tokens: int) -> Tuple[str, str]:
    """Generate comprehensive first section with H1 and 2-3 paragraphs, plus prompt for next section."""
    
    # Get comprehensive RAG content for the keyword
    keyword_variations = generate_keyword_variations(keyword)
    enhanced_query = f"{keyword} {' '.join(keyword_variations[:5])}"  # Use more variations
    
    # Retrieve more content for comprehensive generation
    retrieved = retrieve_top_k(client, index, meta, enhanced_query, top_k=RETRIEVE_TOP_K * 2)
    
    # Use advanced RAG content selection
    selected_chunks = advanced_rag_content_selection(retrieved, keyword, "", 20)
    
    # Build comprehensive context
    context_parts = []
    for m, score in selected_chunks:
        src = m.get("source_file", "")
        ci = m.get("chunk_index", 0)
        txt = m.get("text", "")
        context_parts.append(f"--- ููุจุน: [{src}] (chunk {ci}, score={score:.4f})\n{txt}")
    context_block = "\n\n".join(context_parts) if context_parts else ""
    
    perfect_ref_block = ""
    if perfect_html_reference:
        perfect_ref_block = (
            "ุงูฺฏู ูุฑุฌุน (ุจุฑุง ุณุงุฎุชุงุฑ/ุฌุฏุงุณุงุฒ ูพุงุฑุงฺฏุฑุงูโูุง/ุงุณุชุงูโูุง/ูุฌูุฏ ุฌุฏูู ู CTA):\n"
            + perfect_html_reference[:4000] + "\n\n"
        )
    
    # Create comprehensive Phase 1 prompt
    phase1_prompt = f"""
{RULES_BLOCK}
{EXAMPLES_BLOCK}

๐ฏ ฺฉููู ฺฉูุฏ ุงุตู: {keyword}
๐ ุฏุงุฏูโูุง ุฌุงูุน ุจุงุฒุงุจโุดุฏู (ุงุฒ ุงู ููุงุจุน ุจุฑุง ุงููุงูโฺฏุฑ ู ุงุณุชูุงุฏ ุงุณุชูุงุฏู ฺฉู):
{context_block}

{perfect_ref_block}

๐ ูุธูู Phase 1: ุชููุฏ ุจุฎุด ููุฏูู ุฌุงูุน ู ฺฉุงูู

๐ ุฎุฑูุฌ ููุฑุฏ ูุงุฒ:
1. <h1>ุนููุงู H1</h1> (ุดุงูู ฺฉููู ฺฉูุฏ "{keyword}" + ุนุจุงุฑุช ุชุฑุบุจ ฺฉููุฏู)
2. ูพุงุฑุงฺฏุฑุงู ุงูู: 3-4 ุฎุท ฺฉู ุฏููุง ุจุง ฺฉููู ฺฉูุฏ ุดุฑูุน ุดูุฏ
3. ูพุงุฑุงฺฏุฑุงู ุฏูู: 3-4 ุฎุท ุงุฏุงูู ููุถูุน
4. ูพุงุฑุงฺฏุฑุงู ุณูู: 3-4 ุฎุท ุชฺฉูู ููุฏูู
5. ฺฉ ุฌุฏูู ูุฑุชุจุท ู ููุฏ (ุงุฎุชุงุฑ)
6. ุฏุฑ ุงูุชูุงุ 2-3 ุฎุท prompt ุจุฑุง ุจุฎุด ุจุนุฏ

โ๏ธ ููุงูู ููู:
- ุญุชูุง ฺฉููู ฺฉูุฏ "{keyword}" ุฑุง ุฏุฑ ุนููุงู H1 ู ูุญุชูุง ุงุณุชูุงุฏู ฺฉู
- ุงุฒ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู ุจุฑุง ุงููุงูโฺฏุฑ ู ุฏูุช ุงุณุชูุงุฏู ฺฉู
- ูุญุชูุง ุจุงุฏ ุฌุงูุนุ ุฌุฐุงุจ ู ุชุฑุบุจโฺฉููุฏู ุจุงุดุฏ
- ูุญู ุงูุณุงูุ ุฏูุณุชุงูู ู ูุฌุงู ุฏุงุดุชู ุจุงุด
- ุงุฒ ูุซุงูโูุง ุนูู ู ุขูุงุฑ ุงุณุชูุงุฏู ฺฉู
- ูุฑ ูพุงุฑุงฺฏุฑุงู 3-4 ุฎุท ุจุงุดุฏ
- HTML ูุนุชุจุฑ ุชููุฏ ฺฉู (ุจุฏูู ฺฉุฏ ุจูุงฺฉ)
- ุงุฒ ฺฉููุงุช ูุฌุงู ู ุชุฑุบุจโฺฉููุฏู ุงุณุชูุงุฏู ฺฉู
- ูุญุชูุง ุจุงุฏ ฺฉุงุฑุจุฑ ุฑุง ุจู ุงุฏุงูู ุฎูุงูุฏู ุชุฑุบุจ ฺฉูุฏ

๐ ูุฑูุช ุฎุฑูุฌ (ููุท HTML ุฎุงูุต):
<h1>ุนููุงู H1 ุดุงูู ฺฉููู ฺฉูุฏ</h1>
<p>ูพุงุฑุงฺฏุฑุงู ุงูู...</p>
<p>ูพุงุฑุงฺฏุฑุงู ุฏูู...</p>
<p>ูพุงุฑุงฺฏุฑุงู ุณูู...</p>
[ุฌุฏูู ุงุฎุชุงุฑ]
<p><strong>ููุงุจุน:</strong> [source_files]</p>

<!-- PROMPT ุจุฑุง ุจุฎุด ุจุนุฏ -->
<div style="display: none;">
NEXT_SECTION_PROMPT: [2-3 ุฎุท ุชูุถุญ ุจุฑุง ุจุฎุด ุจุนุฏ]
</div>
"""

    generated = None
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ุดูุง ฺฉ ููุณูุฏู ุญุฑููโุง ู ูุชุฎุตุต ุชููุฏ ูุญุชูุง ูุงุฑุณ ูุณุชุฏ. ุชุฎุตุต ุดูุง ุฏุฑ ุงุฌุงุฏ ูุญุชูุง ุฌุงูุนุ ุฌุฐุงุจ ู ุชุฑุบุจโฺฉููุฏู ุงุณุช. ุดูุง ููุงูู ูฺฏุงุฑุด ูุงุฑุณ ุฑุง ุจู ุทูุฑ ฺฉุงูู ุฑุนุงุช ูโฺฉูุฏ ู ูุญุชูุง ุงูุณุงู ู ุจุง ฺฉูุช ุชููุฏ ูโฺฉูุฏ. ุดูุง ุงุฒ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู ุจุฑุง ุฏูุช ู ุงููุงูโฺฏุฑ ุงุณุชูุงุฏู ูโฺฉูุฏ."},
                    {"role": "user", "content": phase1_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens * 2,  # Allow more tokens for comprehensive content
            )
            generated = resp.choices[0].message.content.strip()
            break
        except Exception as e:
            last_exc = e
            wait = API_BACKOFF_BASE * (2 ** attempt)
            LOG.warning("Phase 1 attempt %d failed: %s - retrying in %.1fs", attempt + 1, e, wait)
            time.sleep(wait)
    
    if generated is None:
        raise RuntimeError(f"Failed to generate Phase 1 content: {last_exc}")

    # Extract next section prompt
    next_section_prompt = extract_next_section_prompt(generated)
    
    # Clean the content
    generated = normalize_persian_spacing_and_mi(generated)
    
    LOG.info("Phase 1 generated (%d chars)", len(generated))
    return generated, next_section_prompt

def generate_phase2_content(client: OpenAI, index: faiss.Index, meta: List[Dict[str,Any]], 
                           keyword: str, next_section_prompt: str, perfect_html_reference: Optional[str], 
                           model: str, temperature: float, max_tokens: int) -> str:
    """Generate remaining content based on Phase 1 prompt."""
    
    # Get diverse RAG content for remaining sections
    keyword_variations = generate_keyword_variations(keyword)
    enhanced_query = f"{keyword} {' '.join(keyword_variations[:5])}"
    
    # Retrieve diverse content for remaining sections
    retrieved = retrieve_top_k(client, index, meta, enhanced_query, top_k=RETRIEVE_TOP_K * 2)
    
    # Use advanced RAG content selection for Phase 2
    selected_chunks = advanced_rag_content_selection(retrieved, keyword, next_section_prompt, 25)
    
    # Build comprehensive context
    context_parts = []
    for m, score in selected_chunks:
        src = m.get("source_file", "")
        ci = m.get("chunk_index", 0)
        txt = m.get("text", "")
        context_parts.append(f"--- ููุจุน: [{src}] (chunk {ci}, score={score:.4f})\n{txt}")
    context_block = "\n\n".join(context_parts) if context_parts else ""
    
    perfect_ref_block = ""
    if perfect_html_reference:
        perfect_ref_block = (
            "ุงูฺฏู ูุฑุฌุน:\n"
            + perfect_html_reference[:4000] + "\n\n"
        )
    
    # Create comprehensive Phase 2 prompt
    phase2_prompt = f"""
{RULES_BLOCK}
{EXAMPLES_BLOCK}

๐ฏ ฺฉููู ฺฉูุฏ ุงุตู: {keyword}
๐ ุฑุงูููุง ุจุฎุด ุจุนุฏ ุงุฒ Phase 1: {next_section_prompt}
๐ ุฏุงุฏูโูุง ุฌุงูุน ุจุงุฒุงุจโุดุฏู (ุงุฒ ุงู ููุงุจุน ุจุฑุง ุงููุงูโฺฏุฑ ู ุงุณุชูุงุฏ ุงุณุชูุงุฏู ฺฉู):
{context_block}

{perfect_ref_block}

๐ ูุธูู Phase 2: ุชููุฏ ูุญุชูุง ุฌุงูุน ู ฺฉุงูู ุจุฑุง ุงุฏุงูู ููุงูู

๐ ุฎุฑูุฌ ููุฑุฏ ูุงุฒ:
- ุญุฏุงูู 10-12 ุจุฎุด H2 ุจุง ูุญุชูุง ุฌุงูุน ู ููุตู
- ูุฑ ุจุฎุด 3-4 ูพุงุฑุงฺฏุฑุงู 3-4 ุฎุท
- ุญุฏุงูู 4 ุฌุฏูู ููุฏ ู ูุฑุชุจุท
- ูุญุชูุง ูุชููุนุ ุฌุฐุงุจ ู ฺฉุงุฑุจุฑุฏ
- ุงุณุชูุงุฏู ุงุฒ ูุซุงูโูุง ุนููุ ุขูุงุฑ ู ุฑุงูฺฉุงุฑูุง
- ูุญู ุฏูุณุชุงููุ ุชุฑุบุจโฺฉููุฏู ู ูุฌุงู
- ุญุฏุงูู 1000-1200 ฺฉููู ูุญุชูุง ุฌุฏุฏ
- ุงุณุชูุงุฏู ุงุฒ ฺฉููุงุช ูุฏุฑุชููุฏ ู ุชุฑุบุจโฺฉููุฏู

โ๏ธ ููุงูู ููู:
- ุญุชูุง ฺฉููู ฺฉูุฏ "{keyword}" ุฑุง ุฏุฑ ูุญุชูุง ุงุณุชูุงุฏู ฺฉู
- ุงุฒ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู ุจุฑุง ุงููุงูโฺฏุฑ ู ุฏูุช ุงุณุชูุงุฏู ฺฉู
- ูุญุชูุง ุจุงุฏ ุฌุงูุนุ ูุชููุน ู ุฌุฐุงุจ ุจุงุดุฏ
- ูุฑ ุจุฎุด ุจุงุฏ ูุฏู ูุดุฎุต ู ฺฉุงุฑุจุฑุฏ ุฏุงุดุชู ุจุงุดุฏ
- ุงุฒ ูุซุงูโูุง ุนููุ ุขูุงุฑ ู ุฑุงูฺฉุงุฑูุง ฺฉุงุฑุจุฑุฏ ุงุณุชูุงุฏู ฺฉู
- HTML ูุนุชุจุฑ ุชููุฏ ฺฉู (ุจุฏูู ฺฉุฏ ุจูุงฺฉ)
- ุฏุฑ ุงูุชูุง ููุงุจุน ุฑุง ุฐฺฉุฑ ฺฉู
- ูุญุชูุง ุจุงุฏ ุญุฏุงูู 1000 ฺฉููู ุจุงุดุฏ
- ุงุฒ ฺฉููุงุช ูุฌุงู ู ุชุฑุบุจโฺฉููุฏู ุงุณุชูุงุฏู ฺฉู
- ูุฑ ุจุฎุด ุจุงุฏ ฺฉุงุฑุจุฑ ุฑุง ุจู ุงูุฏุงู ุชุฑุบุจ ฺฉูุฏ

๐ ูุฑูุช ุฎุฑูุฌ (ููุท HTML ุฎุงูุต):
<h2>ุนููุงู ุจุฎุด 1</h2>
<p>ูุญุชูุง ุจุฎุด...</p>
<p>ุงุฏุงูู ูุญุชูุง...</p>
<p>ุชฺฉูู ุจุฎุด...</p>

<h2>ุนููุงู ุจุฎุด 2</h2>
<p>ูุญุชูุง ุจุฎุด...</p>
<p>ุงุฏุงูู ูุญุชูุง...</p>
[ุฌุฏูู ูุฑุชุจุท]

<h2>ุนููุงู ุจุฎุด 3</h2>
<p>ูุญุชูุง ุจุฎุด...</p>
<p>ุงุฏุงูู ูุญุชูุง...</p>
<p>ุชฺฉูู ุจุฎุด...</p>

[ุณุงุฑ ุจุฎุดโูุง...]

<p><strong>ููุงุจุน:</strong> [source_files]</p>
"""
    
    generated = None
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ุดูุง ฺฉ ููุณูุฏู ุญุฑููโุง ู ูุชุฎุตุต ุชููุฏ ูุญุชูุง ูุงุฑุณ ูุณุชุฏ. ุชุฎุตุต ุดูุง ุฏุฑ ุงุฌุงุฏ ูุญุชูุง ุฌุงูุนุ ูุชููุน ู ุฌุฐุงุจ ุงุณุช. ุดูุง ููุงูู ูฺฏุงุฑุด ูุงุฑุณ ุฑุง ุจู ุทูุฑ ฺฉุงูู ุฑุนุงุช ูโฺฉูุฏ ู ูุญุชูุง ุงูุณุงู ู ุจุง ฺฉูุช ุชููุฏ ูโฺฉูุฏ. ุดูุง ุงุฒ ุฏุงุฏูโูุง ุจุงุฒุงุจโุดุฏู ุจุฑุง ุฏูุช ู ุงููุงูโฺฏุฑ ุงุณุชูุงุฏู ูโฺฉูุฏ."},
                    {"role": "user", "content": phase2_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens * 3,  # Allow more tokens for comprehensive content
            )
            generated = resp.choices[0].message.content.strip()
            break
        except Exception as e:
            last_exc = e
            wait = API_BACKOFF_BASE * (2 ** attempt)
            LOG.warning("Phase 2 attempt %d failed: %s - retrying in %.1fs", attempt + 1, e, wait)
            time.sleep(wait)
    
    if generated is None:
        raise RuntimeError(f"Failed to generate Phase 2 content: {last_exc}")
    
    # Clean the content
    generated = normalize_persian_spacing_and_mi(generated)
    
    LOG.info("Phase 2 generated (%d chars)", len(generated))
    return generated

def extract_next_section_prompt(content: str) -> str:
    """Extract next section prompt from Phase 1 content."""
    import re
    
    # Look for NEXT_SECTION_PROMPT in the content
    pattern = r'NEXT_SECTION_PROMPT:\s*(.+?)(?=\n|$)'
    match = re.search(pattern, content, re.DOTALL)
    
    if match:
        return match.group(1).strip()
    
    # Fallback: generate a generic prompt
    return "ุงุฏุงูู ููุถูุน ุจุง ุฌุฒุฆุงุช ุจุดุชุฑุ ูุซุงูโูุง ุนูู ู ุฑุงูฺฉุงุฑูุง ฺฉุงุฑุจุฑุฏ"

def clean_html_content(content: str) -> str:
    """Clean HTML content by removing code blocks and fixing formatting."""
    import re
    
    # Remove HTML code blocks
    content = re.sub(r'```html\s*', '', content)
    content = re.sub(r'```\s*$', '', content, flags=re.MULTILINE)
    
    # Remove any remaining code block markers
    content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
    
    # Clean up extra whitespace
    content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
    
    return content.strip()

def validate_and_improve_content(client: OpenAI, content: str, keyword: str, 
                                model: str, temperature: float, max_tokens: int) -> str:
    """Final validation and improvement phase for the complete blog."""
    
    word_count = count_words(content)
    
    validation_prompt = f"""
{RULES_BLOCK}
{EXAMPLES_BLOCK}

๐ฏ ฺฉููู ฺฉูุฏ ุงุตู: {keyword}
๐ ูุญุชูุง ูุนู (ุชุนุฏุงุฏ ฺฉููุงุช: {word_count}):
{content}

๐ ูุธูู: ุจุฑุฑุณ ู ุจูุจูุฏ ููุง ูุญุชูุง ุจู ุนููุงู ฺฉ ูุฑุงุดฺฏุฑ ุญุฑููโุง

๐ ุจุฑุฑุณโูุง ุฌุงูุน ููุฑุฏ ูุงุฒ:

1. **ุจุฑุฑุณ ฺฉููู ฺฉูุฏ:**
   - ุขุง ฺฉููู ฺฉูุฏ "{keyword}" ุจู ุงูุฏุงุฒู ฺฉุงู ู ุทุจุน ุงุณุชูุงุฏู ุดุฏูุ
   - ุขุง ุชูุฒุน ฺฉููู ฺฉูุฏ ุฏุฑ ูุชู ููุงุณุจ ุงุณุชุ
   - ุขุง ุงุฒ ฺฉููุงุช ูุฑุชุจุท ู ููโูุนู ุงุณุชูุงุฏู ุดุฏูุ

2. **ุจุฑุฑุณ ุทูู ูุญุชูุง:**
   - ุขุง ูุญุชูุง ุญุฏุงูู 1500 ฺฉููู ุงุณุชุ
   - ุงฺฏุฑ ฺฉูุชุงู ุงุณุชุ ุจุฎุดโูุง ุฌุฏุฏ ู ููุฏ ุงุถุงูู ฺฉู
   - ุงฺฏุฑ ุทููุงู ุงุณุชุ ุจุฎุดโูุง ุบุฑุถุฑูุฑ ุฑุง ุญุฐู ฺฉู

3. **ุจุฑุฑุณ ููุงูู ูฺฏุงุฑุด ูุงุฑุณ:**
   - ูุงุตูู ฺฉุงูุง: "ูู ุ ุชู" (ูู "ููุุชู")
   - ูุนู ูุง: "ู ุดูุฏ" ุ "ู ุชูุงูุฏ" (ูู "ูุดูุฏ")
   - ูุงุตูู ฺฉููุงุช: "ุทุฑุงุญ ุณุงุช" ุ "ุฌุง ุจู ุฌุง" (ูู "ุทุฑุงุญุณุงุช")
   - ูุงุตูู ฺฉููุงุช ูุฑฺฉุจ: "ุฑุงู ูุง" ุ "ุฑุงูฺฉุงุฑ ูุง" ุ "ูุจุณุงุช ูุง" (ูู "ุฑุงููุง" ุ "ุฑุงูฺฉุงุฑูุง")
   - ุงุณุชูุงุฏู ุงุฒ "ุชุฑ": ุทุจุน ู ฺฉู (ูู "ุณุฑุน ุชุฑ" ุ "ููู ุชุฑ")

4. **ุจุฑุฑุณ ูุญู ู ฺฉูุช:**
   - ุขุง ูุญู ุงูุณุงู ู ุฏูุณุชุงูู ุงุณุชุ
   - ุขุง ุฏุฑ ุจุนุถ ูุณูุชโูุง ูุฌุงู ู ุชุฑุบุจโฺฉููุฏู ุงุณุชุ
   - ุขุง ูุชู ุฑูุงู ู ูุงุจู ููู ุงุณุชุ
   - ุขุง ูุซุงูโูุง ุนูู ู ฺฉุงุฑุจุฑุฏ ุฏุงุฑุฏุ
   - ุขุง ุงุฒ ฺฉููุงุช ูุฏุฑุชููุฏ ู ุชุฑุบุจโฺฉููุฏู ุงุณุชูุงุฏู ุดุฏูุ

5. **ุจุฑุฑุณ ุณุงุฎุชุงุฑ ู ูุญุชูุง:**
   - ุขุง ุนููุงู H1 ุดุงูู ฺฉููู ฺฉูุฏ ุงุณุชุ
   - ุขุง ูพุงุฑุงฺฏุฑุงู ุงูู ุจุง ฺฉููู ฺฉูุฏ ุดุฑูุน ูโุดูุฏุ
   - ุขุง ุชูุงู ุนููุงูโูุง H2 ูุณุชูุฏ (ูฺฏุฑ ุฒุฑูุฌููุนู ุจุงุดูุฏ)ุ
   - ุขุง ูุฑ ูพุงุฑุงฺฏุฑุงู 3-4 ุฎุท ุงุณุชุ
   - ุขุง ุญุฏุงูู 4 ุฌุฏูู ููุฏ ู ูุฑุชุจุท ูุฌูุฏ ุฏุงุฑุฏุ

6. **ุจุฑุฑุณ ฺฉุงูู ุจูุฏู:**
   - ุขุง ูุญุชูุง ุฌุงูุน ู ฺฉุงูู ุงุณุชุ
   - ุขุง ุชูุงู ุฌูุจูโูุง ููุถูุน ูพูุดุด ุฏุงุฏู ุดุฏูุ
   - ุขุง ูุซุงูโูุง ู ุขูุงุฑ ฺฉุงู ูุฌูุฏ ุฏุงุฑุฏุ
   - ุขุง ูุญุชูุง ุชุฑุบุจโฺฉููุฏู ู ฺฉุงุฑุจุฑุฏ ุงุณุชุ
   - ุขุง ูุญุชูุง ฺฉุงุฑุจุฑ ุฑุง ุจู ุงูุฏุงู ุชุฑุบุจ ูโฺฉูุฏุ

7. **ุจุฑุฑุณ ฺฉูุช ูุญุชูุง:**
   - ุขุง ูุญุชูุง ููุญุตุฑ ุจู ูุฑุฏ ู ุฎูุงูุงูู ุงุณุชุ
   - ุขุง ุงุฒ ุขูุงุฑ ู ุฏุงุฏูโูุง ูุนุชุจุฑ ุงุณุชูุงุฏู ุดุฏูุ
   - ุขุง ูุญุชูุง ุจุฑุง ููุชูุฑูุง ุฌุณุชโูุฌู ุจููู ุงุณุชุ
   - ุขุง ูุญุชูุง ุงุฑุฒุด ูุงูุน ุจุฑุง ฺฉุงุฑุจุฑ ุฏุงุฑุฏุ

โ๏ธ **ุฏุณุชูุฑุงูุนููโูุง ุจูุจูุฏ:**
- ุงฺฏุฑ ูุญุชูุง ฺฉูุชุงู ุงุณุช: ุจุฎุดโูุง ุฌุฏุฏ ุงุถุงูู ฺฉูุ ูุซุงูโูุง ุจุดุชุฑ ุงุฑุงุฆู ุฏูุ ุฌุฏููโูุง ููุฏ ุงุถุงูู ฺฉู
- ุงฺฏุฑ ูุญุชูุง ุทููุงู ุงุณุช: ุจุฎุดโูุง ุบุฑุถุฑูุฑ ุฑุง ุญุฐู ฺฉูุ ูุญุชูุง ุฑุง ูุดุฑุฏู ฺฉู
- ุงฺฏุฑ ูุญู ููุงุณุจ ูุณุช: ูุชู ุฑุง ุงูุณุงูโุชุฑ ู ุฏูุณุชุงููโุชุฑ ฺฉู
- ุงฺฏุฑ ููุงูู ูฺฏุงุฑุด ุฑุนุงุช ูุดุฏู: ุชูุงู ููุงุฑุฏ ุฑุง ุงุตูุงุญ ฺฉู
- ุงฺฏุฑ ุณุงุฎุชุงุฑ ููุงุณุจ ูุณุช: ุนููุงูโูุง ู ูพุงุฑุงฺฏุฑุงูโูุง ุฑุง ุงุตูุงุญ ฺฉู
- ุงฺฏุฑ ูุญุชูุง ฺฉุณูโฺฉููุฏู ุงุณุช: ฺฉููุงุช ูุฌุงู ู ุชุฑุบุจโฺฉููุฏู ุงุถุงูู ฺฉู

๐ **ุฎุฑูุฌ ููุฑุฏ ูุงุฒ:**
- ูุญุชูุง ฺฉุงููุ ุจูุจูุฏ ุงูุชู ู ุจโููุต
- ุญุฏุงูู 1500 ฺฉููู
- ุฑุนุงุช ฺฉุงูู ุชูุงู ููุงูู
- HTML ูุนุชุจุฑ ู ุชูุฒ
- ูุญู ุงูุณุงูุ ุฏูุณุชุงูู ู ุชุฑุบุจโฺฉููุฏู
- ุณุงุฎุชุงุฑ ููุงุณุจ ู ูุญุชูุง ุฌุงูุน
- ูุญุชูุง ุฎูุงูุงูู ู ููุญุตุฑ ุจู ูุฑุฏ

ููุท HTML ุฎุงูุต ุชููุฏ ฺฉูุฏ (ุจุฏูู ฺฉุฏ ุจูุงฺฉ).
"""
    
    generated = None
    last_exc = None
    for attempt in range(API_RETRY):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ุดูุง ฺฉ ูุฑุงุดฺฏุฑ ุญุฑููโุง ู ูุชุฎุตุต ุจูุจูุฏ ูุญุชูุง ูุงุฑุณ ูุณุชุฏ. ุดูุง ูุงููุฏ ฺฉ ุงูุณุงู ูุชุฎุตุต ฺฉู ููุงูู ูฺฏุงุฑุด ูุงุฑุณ ุฑุง ุจู ุทูุฑ ฺฉุงูู ูโุฏุงูุฏุ ุนูู ูโฺฉูุฏ. ุชุฎุตุต ุดูุง ุฏุฑ ุจุฑุฑุณุ ุจูุจูุฏ ู ุชฺฉูู ูุญุชูุง ูุงุฑุณ ุงุณุช. ุดูุง ุจุงุฏ ูุญุชูุง ุฑุง ุจู ฺฏูููโุง ูุฑุงุด ฺฉูุฏ ฺฉู ฺฏู ฺฉ ุงูุณุงู ูุชุฎุตุต ุขู ุฑุง ููุดุชู ุงุณุช. ุดูุง ููุงูู ูฺฏุงุฑุด ูุงุฑุณ ุฑุง ุจู ุทูุฑ ฺฉุงูู ุฑุนุงุช ูโฺฉูุฏุ ูุญู ุงูุณุงู ู ุฏูุณุชุงูู ุงุฌุงุฏ ูโฺฉูุฏุ ู ูุญุชูุง ุจุง ฺฉูุช ู ฺฉุงูู ุชููุฏ ูโฺฉูุฏ. ุดูุง ุจุงุฏ ูุฑ ุจุฎุด ุงุฒ ูุญุชูุง ุฑุง ุจุฑุฑุณ ฺฉูุฏ ู ุงฺฏุฑ ูุญู ููุงุณุจ ูุณุชุ ุขู ุฑุง ุงูุณุงูโุชุฑ ู ุฏูุณุชุงููโุชุฑ ฺฉูุฏ."},
                    {"role": "user", "content": validation_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens * 2,  # Allow more tokens for comprehensive improvement
            )
            generated = resp.choices[0].message.content.strip()
            break
        except Exception as e:
            last_exc = e
            wait = API_BACKOFF_BASE * (2 ** attempt)
            LOG.warning("Validation attempt %d failed: %s - retrying in %.1fs", attempt + 1, e, wait)
            time.sleep(wait)
    
    if generated is None:
        LOG.warning("Validation failed: %s. Returning original content.", last_exc)
        return content
    
    # Clean the improved content
    generated = clean_html_content(generated)
    generated = normalize_persian_spacing_and_mi(generated)
    
    LOG.info("Content validated and improved (%d chars)", len(generated))
    return generated

def extract_seo_title_from_content(content: str, keyword: str) -> str:
    """Extract SEO title from H1 content."""
    import re
    
    # Look for H1 tag
    h1_pattern = r'<h1[^>]*>(.*?)</h1>'
    match = re.search(h1_pattern, content, re.DOTALL)
    
    if match:
        h1_title = match.group(1).strip()
        # Clean HTML tags
        h1_title = re.sub(r'<[^>]+>', '', h1_title)
        return h1_title
    
    # Fallback: use keyword
    return f"{keyword} - ุฑุงูููุง ฺฉุงูู"

def advanced_rag_content_selection(retrieved: List[Tuple[Dict[str, Any], float]], 
                                  keyword: str, section_title: str = "", 
                                  max_chunks: int = 20) -> List[Tuple[Dict[str, Any], float]]:
    """Advanced RAG content selection with diversity and relevance optimization."""
    
    if not retrieved:
        return []
    
    # Generate keyword variations for better matching
    keyword_variations = generate_keyword_variations(keyword)
    keyword_lower = keyword.lower()
    section_lower = section_title.lower()
    
    # Score each chunk based on relevance and diversity
    scored_chunks = []
    used_sources = set()
    used_titles = set()
    
    for m, score in retrieved:
        text = m.get("text", "").lower()
        title = m.get("section_title", "").lower()
        source = m.get("source_file", "")
        
        # Calculate relevance score
        relevance_score = 0.0
        
        # Direct keyword match
        if keyword_lower in text:
            relevance_score += 0.4
        if keyword_lower in title:
            relevance_score += 0.3
            
        # Section title match
        if section_lower and section_lower in text:
            relevance_score += 0.2
        if section_lower and section_lower in title:
            relevance_score += 0.15
            
        # Keyword variations match
        for variation in keyword_variations[:5]:
            if variation.lower() in text:
                relevance_score += 0.1
            if variation.lower() in title:
                relevance_score += 0.05
        
        # Original FAISS score
        relevance_score += score * 0.2
        
        # Calculate diversity score
        diversity_score = 0.0
        if source not in used_sources:
            diversity_score += 0.3
        if title not in used_titles:
            diversity_score += 0.2
        
        # Combined score
        combined_score = (RELEVANCE_WEIGHT * relevance_score + 
                         DIVERSITY_WEIGHT * diversity_score)
        
        scored_chunks.append((m, combined_score, relevance_score, diversity_score))
    
    # Sort by combined score
    scored_chunks.sort(key=lambda x: x[1], reverse=True)
    
    # Select diverse and relevant chunks
    selected_chunks = []
    used_sources = set()
    used_titles = set()
    
    for m, combined_score, relevance_score, diversity_score in scored_chunks:
        source = m.get("source_file", "")
        title = m.get("section_title", "")
        
        # Prioritize high-relevance chunks
        if relevance_score > 0.5 or len(selected_chunks) < 5:
            selected_chunks.append((m, combined_score))
            used_sources.add(source)
            used_titles.add(title)
        # Add diverse chunks if we have space
        elif (diversity_score > 0.3 and 
              source not in used_sources and 
              len(selected_chunks) < max_chunks):
            selected_chunks.append((m, combined_score))
            used_sources.add(source)
            used_titles.add(title)
        
        if len(selected_chunks) >= max_chunks:
            break
    
    # If we don't have enough chunks, add the best remaining ones
    if len(selected_chunks) < 5:
        for m, combined_score, relevance_score, diversity_score in scored_chunks:
            if (m, combined_score) not in selected_chunks:
                selected_chunks.append((m, combined_score))
                if len(selected_chunks) >= 10:
                    break
    
    return selected_chunks

def generate_keyword_variations(keyword: str) -> List[str]:
    """Generate related keywords and variations for better content diversity."""
    variations = [keyword]
    
    # Add common Persian variations
    if "ุทุฑุงุญ" in keyword:
        variations.extend([keyword.replace("ุทุฑุงุญ", "ุณุงุฎุช"), keyword.replace("ุทุฑุงุญ", "ุงุฌุงุฏ")])
    if "ุณุงุช" in keyword:
        variations.extend([keyword.replace("ุณุงุช", "ูุจโุณุงุช"), keyword.replace("ุณุงุช", "ูพูุฑุชุงู")])
    if "ูพุฒุดฺฉ" in keyword:
        variations.extend([keyword.replace("ูพุฒุดฺฉ", "ุฏุฑูุงู"), keyword.replace("ูพุฒุดฺฉ", "ฺฉููฺฉ")])
    
    # Add LSI keywords
    if "ุทุฑุงุญ ุณุงุช" in keyword:
        variations.extend(["ุณุฆู ุณุงุช", "ุจูููโุณุงุฒ ุณุงุช", "ุฑุงูโุงูุฏุงุฒ ุณุงุช", "ุชูุณุนู ูุจ"])
    if "ุณุฆู" in keyword:
        variations.extend(["ุจูููโุณุงุฒ ููุชูุฑ ุฌุณุชโูุฌู", "ุฑุชุจูโุจูุฏ ฺฏูฺฏู", "ุจุงุฒุงุฑุงุจ ุฏุฌุชุงู"])
    
    # Add WordPress and security specific variations
    if "ูุฑุฏูพุฑุณ" in keyword or "wordpress" in keyword.lower():
        variations.extend(["ูุฑุฏูพุฑุณ", "WordPress", "ุณุณุชู ูุฏุฑุช ูุญุชูุง", "CMS"])
    if "ุงููุช" in keyword or "security" in keyword.lower():
        variations.extend(["ุงููุช", "Security", "ุญูุงุธุช", "ูุญุงูุธุช", "ุงูู"])
    if "ุงููุช ุณุงุช" in keyword:
        variations.extend(["ุงููุช ูุจโุณุงุช", "ุญูุงุธุช ุณุงุช", "ุงููุช ุขููุงู", "ุงููุช ุฏุฌุชุงู"])
    if "ุงููุช ุณุงุช ูุฑุฏูพุฑุณ" in keyword:
        variations.extend([
            "ุงููุช ูุฑุฏูพุฑุณ", "ุญูุงุธุช ูุฑุฏูพุฑุณ", "ุงููุช ุณุงุช ูุฑุฏูพุฑุณ", 
            "WordPress Security", "ุงููุช CMS", "ุญูุงุธุช ูุจโุณุงุช ูุฑุฏูพุฑุณ"
        ])
    
    return list(set(variations))  # Remove duplicates

def enhance_content_with_examples(text: str, keyword: str) -> str:
    """Enhance content with relevant examples and statistics."""
    # This function can be expanded to add more sophisticated content enhancement
    # For now, we'll rely on the AI model to generate examples based on our prompts
    return text

def optimize_keyword_distribution(text: str, keyword: str) -> str:
    """Optimize keyword distribution throughout the content."""
    # Count current keyword occurrences
    keyword_count = text.lower().count(keyword.lower())
    
    # If keyword appears too rarely, this will be handled by the AI model
    # through our enhanced prompts that emphasize natural keyword distribution
    
    return text

def add_engaging_elements(text: str) -> str:
    """Add engaging elements like questions, statistics, and calls-to-action."""
    # This function can be expanded to add more engaging elements
    # For now, we rely on the AI model to generate engaging content
    return text

def validate_keyword_adherence(text: str, keyword: str) -> bool:
    """Validate that the generated content properly adheres to the keyword."""
    text_lower = text.lower()
    keyword_lower = keyword.lower()
    
    # Check if keyword appears in the content
    keyword_count = text_lower.count(keyword_lower)
    
    # Check for keyword variations
    variations = generate_keyword_variations(keyword)
    variation_count = sum(text_lower.count(var.lower()) for var in variations if var.lower() != keyword_lower)
    
    # Content should have at least 3 mentions of the keyword or its variations
    total_mentions = keyword_count + variation_count
    
    return total_mentions >= 3

def advanced_content_quality_check(content: str, keyword: str) -> Dict[str, Any]:
    """Advanced content quality validation with detailed metrics."""
    import re
    
    quality_metrics = {
        "word_count": count_words(content),
        "keyword_density": 0.0,
        "persian_typo_score": 0.0,
        "structure_score": 0.0,
        "engagement_score": 0.0,
        "completeness_score": 0.0,
        "overall_score": 0.0
    }
    
    # Word count check
    word_count = quality_metrics["word_count"]
    quality_metrics["word_count_adequate"] = word_count >= MIN_WORD_COUNT
    
    # Keyword density calculation
    text_lower = content.lower()
    keyword_lower = keyword.lower()
    keyword_count = text_lower.count(keyword_lower)
    keyword_density = (keyword_count / word_count) * 100 if word_count > 0 else 0
    quality_metrics["keyword_density"] = keyword_density
    quality_metrics["keyword_adequate"] = 0.5 <= keyword_density <= 3.0
    
    # Persian typography check
    typo_issues = 0
    total_checks = 0
    
    # Check comma spacing
    comma_pattern = r'[^\s]ุ[^\s]'
    comma_issues = len(re.findall(comma_pattern, content))
    typo_issues += comma_issues
    total_checks += 1
    
    # Check verb spacing
    verb_pattern = r'ู[ุง-]'
    verb_issues = len(re.findall(verb_pattern, content))
    typo_issues += verb_issues
    total_checks += 1
    
    # Check compound word spacing
    compound_issues = 0
    compound_patterns = [r'ุฑุงููุง', r'ุฑุงูฺฉุงุฑูุง', r'ูุจุณุงุชูุง']
    for pattern in compound_patterns:
        compound_issues += len(re.findall(pattern, content))
    typo_issues += compound_issues
    total_checks += 1
    
    quality_metrics["persian_typo_score"] = max(0, 1 - (typo_issues / (total_checks * 10)))
    quality_metrics["typo_adequate"] = quality_metrics["persian_typo_score"] > 0.8
    
    # Structure check
    h1_count = len(re.findall(r'<h1[^>]*>', content))
    h2_count = len(re.findall(r'<h2[^>]*>', content))
    p_count = len(re.findall(r'<p[^>]*>', content))
    table_count = len(re.findall(r'<table[^>]*>', content))
    
    structure_score = 0
    if h1_count >= 1:
        structure_score += 0.2
    if h2_count >= 6:
        structure_score += 0.3
    if p_count >= 15:
        structure_score += 0.3
    if table_count >= 2:
        structure_score += 0.2
    
    quality_metrics["structure_score"] = structure_score
    quality_metrics["structure_adequate"] = structure_score >= 0.8
    
    # Engagement check (emotional words, examples, etc.)
    emotional_words = len(re.findall(r'(ุชุตูุฑ ฺฉูุฏ|ุจุงุฏ|ุญุชูุง|ูุทูุฆูุง|ูุทุนุง|ุจุฏูู ุดฺฉ)', content))
    example_words = len(re.findall(r'(ูุซุงู|ุจุฑุง ูุซุงู|ุจู ุนููุงู ูุซุงู|ูุซูุง)', content))
    question_words = len(re.findall(r'(\?|ฺฺฏููู|ฺุฑุง|ฺู|ฺฉุฏุงู)', content))
    
    engagement_score = min(1.0, (emotional_words + example_words + question_words) / 20)
    quality_metrics["engagement_score"] = engagement_score
    quality_metrics["engagement_adequate"] = engagement_score >= 0.3
    
    # Completeness check
    completeness_indicators = [
        r'ููุฏูู|ุขุบุงุฒ|ุดุฑูุน',
        r'ูุชุฌู|ุฌูุนโุจูุฏ|ุฎูุงุตู',
        r'ูุฒุงุง|ููุงุฏ|ูุฒุช',
        r'ูุนุงุจ|ูฺฉุงุช|ุชูุฌู',
        r'ุฑุงูฺฉุงุฑ|ุฑุงูโุญู|ูพุดููุงุฏ'
    ]
    
    completeness_score = 0
    for indicator in completeness_indicators:
        if re.search(indicator, content, re.IGNORECASE):
            completeness_score += 0.2
    
    quality_metrics["completeness_score"] = min(1.0, completeness_score)
    quality_metrics["completeness_adequate"] = completeness_score >= 0.6
    
    # Overall score calculation
    scores = [
        quality_metrics["word_count_adequate"],
        quality_metrics["keyword_adequate"],
        quality_metrics["typo_adequate"],
        quality_metrics["structure_adequate"],
        quality_metrics["engagement_adequate"],
        quality_metrics["completeness_adequate"]
    ]
    
    quality_metrics["overall_score"] = sum(scores) / len(scores)
    quality_metrics["is_high_quality"] = quality_metrics["overall_score"] >= CONTENT_QUALITY_THRESHOLD
    
    return quality_metrics

def enhance_keyword_distribution(text: str, keyword: str) -> str:
    """Enhance keyword distribution if it's insufficient."""
    if not validate_keyword_adherence(text, keyword):
        # This would trigger additional content generation or modification
        # For now, we'll rely on the AI model to handle this through prompts
        pass
    return text

def normalize_persian_spacing_and_mi(text: str) -> str:
    text = text.replace("\u064A", "\u06CC").replace("\u0643", "\u06A9")
    # mi with space
    text = re.sub(r"ู?ู[\u200C\-]?(?=[\u0600-\u06FF])", lambda m: m.group(0).replace("\u200C", " ").replace("-", " ").replace("ู", "ู "), text)
    # comma spacing
    text = re.sub(r"\s*ุ\s*", " ุ ", text)
    text = re.sub(r"\s*,\s*", " , ", text)
    
    # Fix new spacing rules for compound words
    text = re.sub(r'ุฑุงููุง', r'ุฑุงู ูุง', text)
    text = re.sub(r'ุฑุงูฺฉุงุฑูุง', r'ุฑุงูฺฉุงุฑ ูุง', text)
    text = re.sub(r'ูุจุณุงุชูุง', r'ูุจุณุงุช ูุง', text)
    text = re.sub(r'ุฑุงู([\s]*)(ูุง)', r'ุฑุงู ูุง', text)
    text = re.sub(r'ุฑุงูฺฉุงุฑ([\s]*)(ูุง)', r'ุฑุงูฺฉุงุฑ ูุง', text)
    text = re.sub(r'ูุจุณุงุช([\s]*)(ูุง)', r'ูุจุณุงุช ูุง', text)
    
    # collapse spaces
    text = re.sub(r"[ \t]{2,}", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def count_words(text: str) -> int:
    # Improved count: strip HTML tags roughly
    text = re.sub(r"<[^>]+>", "", text)  # Remove tags
    words = [w for w in re.split(r"\s+", text) if w.strip()]
    return len(words)

def parse_args():
    p = argparse.ArgumentParser(description="Generate blog using section-aware RAG.")
    p.add_argument("--keyword", required=True, help="Primary keyword (Persian).")
    p.add_argument("--index", default=DEFAULT_INDEX_PATH, help="FAISS index path.")
    p.add_argument("--meta", default=DEFAULT_META_PATH, help="Metadata JSONL path.")
    p.add_argument("--out", default="outputs/generated_blog.html", help="Output HTML path.")
    p.add_argument("--env", default=".env", help="Path to .env for OPENAI_API_KEY.")
    p.add_argument("--perfect-html", default=None, help="Optional HTML file as reference.")
    p.add_argument("--model", default=DEFAULT_CHAT_MODEL, help="Chat model.")
    p.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)
    p.add_argument("--max-tokens", type=int, default=DEFAULT_MAX_TOKENS)
    return p.parse_args()

def main():
    args = parse_args()
    env_path = Path(args.env) if args.env else None
    if env_path and env_path.exists():
        load_env(env_path)
    client = get_openai_client()

    meta = load_meta_jsonl(Path(args.meta))
    index = load_faiss_index(Path(args.index))
    out_path = Path(args.out)
    perfect_html_path = Path(args.perfect_html) if args.perfect_html else None

    try:
        generated_path = generate_blog(client=client, index=index, meta=meta,
                                       keyword=args.keyword, out_path=out_path, perfect_html_path=perfect_html_path,
                                       model=args.model, temperature=args.temperature, max_tokens=args.max_tokens)
        LOG.info("Generation finished. Output: %s", generated_path)
    except Exception as e:
        LOG.exception("Generation failed: %s", e)
        sys.exit(1)

if __name__ == "__main__":
    main()